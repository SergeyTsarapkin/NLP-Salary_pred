{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843109b3",
   "metadata": {},
   "source": [
    "### Цель проекта: построить модель, которая сможет предсказывать зарплату по текстовому описанию вакансии.\n",
    "\n",
    "Сам датасет взят из соревнования \"2nd step in NLP\", которое недавно поводила ВШЭ.\n",
    "В бейзлайне который был предоставлен организаторами использовался Gensim'овский НЕ-предобученный Word2Vec, который обучили на колонке с описаниями вакансий, получив эмбеддинги для каждого слова и усреднив их, тем самым получив W2V всего описания вакансии. Далее для предсказания зарплаты использовали обычную линейную регрессию с L1 регуляризатором. \n",
    "\n",
    "Я попробую нсколько подходов, состоящих из комбинации способа получения вектора текста и предсказательной модели:\n",
    "Получение векторов:\n",
    "- различные вариации TF-IDF\n",
    "- Word2Vec\n",
    "\n",
    "Предсказательная модель:\n",
    "- Регресия с различными вариантами регуляризации\n",
    "- Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186d721a",
   "metadata": {},
   "source": [
    "#### Описание датасета: \n",
    "Датасет сождержит 16231 уникальных строк с данными по вакансиям, а именно:\n",
    "- название вакансии\n",
    "- режим работы\n",
    "- тип занятости\n",
    "- описание вакансии\n",
    "- требуемые навыки\n",
    "- зарплата\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 8]\n",
    "\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import catboost\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "import random\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)\n",
    "rng = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('vacancies_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b405946",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5db1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5623cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f49b0d",
   "metadata": {},
   "source": [
    "Пропусков нет, кроме колонки \"требуемые навыки\", но она нас не так сильно волнует."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375490e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим есть ли задвоения строк:\n",
    "df[df.duplicated(keep=False)].sort_values(by='salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a540fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалим дубликаты оставив первые вхождения, и посмотрим сколько осталось:\n",
    "df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409af59",
   "metadata": {},
   "source": [
    "### EDA and Text Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f9fd0",
   "metadata": {},
   "source": [
    "Перед тем как переходить к обработке текста, посмотрим что вообще из себя представляют данные, для начала визуально посмотрим какие профессии больше всего ищут:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stop_words, background_color='white', width=1400, height=800).generate_from_frequencies(df.name.value_counts())\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646219f",
   "metadata": {},
   "source": [
    "Бегло посмотрим на рпаспределение типов занятостии графика работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.xticks(rotation=25)\n",
    "sns.histplot(df['schedule'], color='#DAF7A6', shrink=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940a73e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.xticks(rotation=25)\n",
    "sns.histplot(df['employment'], color='#DAF7A6', shrink=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e4f6e",
   "metadata": {},
   "source": [
    "Вполне ожидаемо, абсолютно большая часть - полный день + полная занятость. \\\n",
    "Посмотрим на распределение целевой переменной - зарплаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bde3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['salary'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90253fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.salary, bins = 100, rwidth=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ecac08",
   "metadata": {},
   "source": [
    "Похоже на лог-нормальное распределение, попробуем отлогарифмировать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c944a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(df.salary), bins = 100, rwidth=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf75b4",
   "metadata": {},
   "source": [
    "А что там с совместным распределением зарплат по типу занятости опыту?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe4beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut=0 чтобы на графике отображались только днаные из датасета, без сглаживания,\n",
    "# которое может визуально показать отрицательную зарплату.\n",
    "# scale='area' чтобы нивелировать разницу в частоте видов занятости.\n",
    "sns.violinplot(data=df, y=df['salary']/1000, x='schedule', hue='employment',\n",
    "               scale='area', cut=0) \n",
    "plt.ylabel('salary (1,000s)')\n",
    "plt.ylim(-10, 250); # отрезаем зарплты 250+, поскольку они являются выбросами,\n",
    "                   # и визуально только помешают оценить взаимсвязи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6bdcc1",
   "metadata": {},
   "source": [
    "Четкую зависимость установить трудно, разве что вахтовый метод выбивается в большую сторону, что вполне логично, перекодировать в числительный не будем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dfa37f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot(data=df, y=df['salary']/1000, x='experience', hue='schedule',\n",
    "               order=df.experience.value_counts().keys(), scale='area', cut=0) \n",
    "plt.ylabel('salary (1,000s)')\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad28ed",
   "metadata": {},
   "source": [
    "Тут пару выводов выводов: \\\n",
    "1 - четко виден тренд, что зарплата растет в зависимости от требуемого опыта, поэтому будем перекодировать признак опыта на числительный 1-4, где 1 - \"нет опыта\", а 4 - \"более 6 лет\". \\\n",
    "2 - огромные выбросы в большую сторону, даже там где опыт не требуется, подозреваю что это вакансии связанные с продажами, с низким окладом, и \"безграничными возможностями заработать миллиарды\" как % от продаж. И небольшие (логично ограниченные нулем) выбросы в меньшую сторону.\n",
    "\n",
    "#### Я не хочу удалять выбросы в большую сторону, так как это приведет к потере части информации, выбросы в меньшую сторону обусловлены почасовой или посменной оплатой, хочется исправить, но это крайне проблемматично, поскольку потребует ручных правок, если их не очень много, то оставлю как есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.salary < 10000].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9be5d5",
   "metadata": {},
   "source": [
    "59 вакансий с зарплатой меньше 10 тыс, выборосы в меньшую оставляем как есть.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda27699",
   "metadata": {},
   "source": [
    "#### Предобработка:\n",
    "Перед тем как преобразовывать текст в вектора методами вроде TFIDF / Word2Vec, его нужно подготовить, очистить от знаков препинания, лишних символов, стоп слов, привести слова к нормальной форме. \\\n",
    "Для очистки напишем несколько функций:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44685a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим новый датафрейм, где будут очищенные данные:\n",
    "df_p = pd.DataFrame() # p - processed\n",
    "\n",
    "# Создадим функцию для замены признака опыта на количественный:\n",
    "def conditions(x):\n",
    "    if x == 'Нет опыта': return 1\n",
    "    elif x == 'От 1 года до 3 лет': return 2\n",
    "    elif x == 'От 3 до 6 лет': return 3\n",
    "    else: return 4\n",
    "\n",
    "conditions_vect = np.vectorize(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f909219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# мусорные, не несущие особого смысла слова, кандидаты на удаление,\n",
    "#stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "print('Примеры стоп слов:')\n",
    "for i in range(3):\n",
    "    print(list(stop_words)[i])\n",
    "print(f'всего стоп слов: {len(stop_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ad1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для чистки текстов (описаний) от разного регистра, и всякого мусора\n",
    "# вроде стопслов или не текстовых значений\n",
    "def initial_processing(corpus):\n",
    "    n = len(corpus)\n",
    "    for i in range(n):\n",
    "        corpus[i] = corpus[i].lower()\n",
    "        corpus[i] = re.sub(r'[^а-яА-Яa-zA-Z0-9]', ' ', corpus[i])\n",
    "        corpus[i] = [word for word in corpus[i].split() if word not in stop_words and len(word) > 1]\n",
    "        corpus[i] = ' '.join(corpus[i])\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для примера:\n",
    "# текст описания ДО:\n",
    "example_text = df['description'][0:2].copy()\n",
    "example_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# текст описания ПОСЛЕ:\n",
    "a = initial_processing(example_text)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb288aa5",
   "metadata": {},
   "source": [
    "Будем использовать признаки:\n",
    " - название вакасии\n",
    " - описание вакансии\n",
    " - требуемый опыт\n",
    " - ключевые навыки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500fb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p['name'] = initial_processing(df['name'].copy())\n",
    "df_p['description'] = initial_processing(df['description'].copy())\n",
    "df_p['experience'] = conditions_vect(df['experience'])\n",
    "df_p['key_skills'] = initial_processing(df['key_skills'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e644d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "\n",
    "# Принимаем слово, возвращаем нормальную форму,\n",
    "# также используем кэширование для ускорения (много одинаковых итераций):\n",
    "@lru_cache(maxsize=256)\n",
    "def lemmatize_word(word):\n",
    "    return morph.parse(word)[0].normal_form \n",
    "\n",
    "# Пробегаемся по корпусу, в каждом куске, пробегаемся по всем словам, передавая их в функцию выше\n",
    "# и сохраняем преобразованные параграфы:\n",
    "def lemmatize(corpus):\n",
    "    n = len(corpus)\n",
    "    with tqdm(total=n) as pbar:\n",
    "        for i in range(n):\n",
    "            corpus[i] = ' '.join([lemmatize_word(word) for word in corpus[i].split()])\n",
    "            pbar.update(1)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d076fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p['name'] = lemmatize(df_p['name'].copy())\n",
    "df_p['description'] = lemmatize(df_p['description'].copy())\n",
    "df_p['key_skills'] = lemmatize(df_p['key_skills'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe13d3",
   "metadata": {},
   "source": [
    "Готово, теперь перейдем разделим данные на тренировочные и тестовые, и перейдем к 2 главным частям, векторизации и построению предсказательной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['salary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_p, y, test_size=0.25, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf15d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb03ae6",
   "metadata": {},
   "source": [
    "#### Небольшой дисклеймер: этапом создания новых признаков буду считать векторизацию, так как это и есть создание признаков, которые пойдут на вход модели предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07cb146",
   "metadata": {},
   "source": [
    "### Baseline: TF-IDF векторизация описания вакансии с дефолтными параметрами + линейная регрессия со стохастическим градиентным спуском\n",
    "Коротко про TF-IDF, это скажем так развитие Bag of Words (BoW). В BoW мы создаем словарь из всех уникальных токенов (слов) во всем нашем тексте (корпусе), и дальше можем каждому отрывку общего текста (предложению, параграфу) присвоить вектор длинны словаря, где каждый элемент - количество раз, сколько конкретное слово из словаря встречается в этом отрывке.\\\n",
    "\\\n",
    "Это самый простой способ превратить текст в цифры для понимания компьютера. Но и самый бесполезный, никак не учитывающий даже важность слов, не говоря уже о порядке слов и семантике. Также, при большом корпусе, словарь раздувается до гигантских размеров, особенно если составлять его не только из еденичных слов, но и н-грамм. А вектора для отрывков будут на 99% состоять из нулей.\\\n",
    "\\\n",
    "Н-граммы это последовательности из слов, например можно рассматривать \"Высшая Школа Экономики\" как 3 отдельных токена, или объеденить их в одну триграмму. в первом случае, предложения \"В школе преподают экономику на высшем уровне\" и \"В Высшей Школе Экономики кофе делают на уровне\" будут иметь довольно схожие вектора, а вот при добавлении н-грамм появится больше различий.\\\n",
    "\\\n",
    "TF-IDF это попытка учесть важность слов, мы все еще создаем общий словарь, и каждому отрывку (предложению) присваиваем вектор длинны словаря, но значения в этом векторе, это уже не просто количество вхождений. Они считаются так: \n",
    "\n",
    "$$TF = \\frac{сколько\\ раз\\ слово\\ вошло\\ в\\ отрывок}{сколько\\ слов\\ во\\ всем\\ корпусе}$$\n",
    "\n",
    "$$IDF = log(\\frac{сколько\\ отрывков\\ в\\ корпусе}{сколько\\ отрывков\\ в\\ корпусе\\ которые\\ содержат\\ данное\\ слово})$$\n",
    "$$ TF-IDF = TF * IDF $$\n",
    "Смысл тут в том, что если некое слово часто встречается в отрывке (параграфе), но при этом крайне редко встречается в остальном корпусе, оно видимо несет важную информацию, а вот если оно встречается везде, то видимо оно ничего особо не значит (самые частые слова, например \"что, и, от, из\" вообзе можно удалить, приняв за \"стоп слова\", что мы и сделали на этапе предварительной обработки). \\\n",
    "\\\n",
    "BoW в чистом виде я не вижу смысла использовать, сразу возьмем TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для иллюстрации размеров BoW\n",
    "count = CountVectorizer()\n",
    "X_count = count.fit(X_train['description'])\n",
    "\n",
    "count_ngram = CountVectorizer(ngram_range=(1,2))\n",
    "X_count_ngram = count_ngram.fit(X_train['description'])\n",
    "\n",
    "print(f'длинна словаря только с юниграммами: {len(X_count.vocabulary_)}')\n",
    "print(f'длинна словаря с биграммами: {len(X_count_ngram.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae692e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tfidf = TfidfVectorizer() #только юниграмы\n",
    "X_train_tfidf = baseline_tfidf.fit_transform(X_train['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5b911",
   "metadata": {},
   "source": [
    "В результате мы получили разряженную матрицу (для экономии памяти, так как большинство значений все равно 0), каждя строка которой - описание вакансии представленное в виде TF-IDF вектора с длинной равной количеству слов в общем словаре (33366)\\\n",
    "\\\n",
    "Теперь построим предсказательную модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reg = SGDRegressor(max_iter=2000, learning_rate='adaptive', penalty = 'l2', random_state=rng)\n",
    "reg.fit(X_train_tfidf, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# кол-во итераций подобрано импирически, с адаптивным learning-rate и l2 регуляризатором\n",
    "# модель достаточно быстро сходится, ниже реальное кол-во итераций пройденных моделью\n",
    "# запас оставлен для кросс валидации\n",
    "reg.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = baseline_tfidf.transform(X_test['description'])\n",
    "pred = reg.predict(X_test_tfidf)\n",
    "baseline_r2 = r2_score(y_test, pred)\n",
    "baseline_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c8501",
   "metadata": {},
   "source": [
    "Это очень хороший, в бейзлайне самого соревнования, они получили чтото около 0.23. Первое место заняла модель с 0.51. Сразу же отмечу, что я регрессия с l-2 регуляризацией гораздо лучше себя показывает чем с l1 или вообще без нее. Эти варианты я пробовал, но отбросил сразу. Сделаем кросс-валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(reg, X_train_tfidf, y_train,\n",
    "                         scoring='r2', cv=10, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086993a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cv = np.mean(scores)\n",
    "scores, baseline_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a94af",
   "metadata": {},
   "source": [
    "Очень неплохо для бейзлайна. Посмотрим что можно улучшить.\n",
    "___\n",
    "#### Оптимизация:\n",
    "Я разделю оптимизацию на 2 части, улучшение векторизации, и моделирование, сначала, используя бейзлайновую модель регрессии, попробуем улучшить входные данные, поменяв векторизацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем использовать 3 дополнительные колонки: название, ключевые навыки и опыт работы:\n",
    "\n",
    "vect_desc, vect_name, vect_skills = TfidfVectorizer(), TfidfVectorizer(), TfidfVectorizer()\n",
    "\n",
    "X_train_desc = vect_desc.fit_transform(X_train['description'])\n",
    "X_train_name = vect_name.fit_transform(X_train['name'])\n",
    "X_train_skills = vect_skills.fit_transform(X_train['key_skills'])\n",
    "X_train_exp = scipy.sparse.csc_matrix(X_train['experience']).transpose()\n",
    "\n",
    "X_train_tfidf = hstack([X_train_desc, X_train_name, X_train_skills, X_train_exp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ecd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db153398",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = SGDRegressor(max_iter=30000, learning_rate='adaptive', penalty = 'l2', random_state=rng)\n",
    "reg.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511083fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_desc = vect_desc.transform(X_test['description'])\n",
    "X_test_name = vect_name.transform(X_test['name'])\n",
    "X_test_skills = vect_skills.transform(X_test['key_skills'])\n",
    "X_test_exp = scipy.sparse.csc_matrix(X_test['experience']).transpose()\n",
    "\n",
    "X_test_tfidf = hstack([X_test_desc, X_test_name, X_test_skills, X_test_exp])\n",
    "\n",
    "pred = reg.predict(X_test_tfidf)\n",
    "all_text_r2 = r2_score(y_test, pred)\n",
    "all_text_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0c6e1",
   "metadata": {},
   "source": [
    "Небольшое улучшение, посмотрим что будет на кросс валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c917c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(reg, X_train_tfidf, y_train,\n",
    "                         scoring='r2', cv=5, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_cv = np.mean(scores)\n",
    "scores, all_text_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77942da",
   "metadata": {},
   "source": [
    "Так себе, попробуем перебрать вариации н-грамм, пока будем идти только по словам, вариацию с анализатором отдельных буквенных сочетаний сделаем дальше, также добавим ограничение на минимальное количество слов в корпусе для словаря, чтобы сильно сократить его размер за счет слов, встретившихся 1 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_gram = [(1,1), (1, 2), (1, 3), (1,4)]\n",
    "best_ngram = 0\n",
    "best_r2 = 0\n",
    "for n in n_gram:\n",
    "    vect_desc_n = TfidfVectorizer(ngram_range=n, min_df=2)\n",
    "    vect_name_n = TfidfVectorizer(ngram_range=n, min_df=2)\n",
    "    vect_skills_n = TfidfVectorizer(ngram_range=n, min_df=2)\n",
    "    \n",
    "    X_train_desc = vect_desc_n.fit_transform(X_train['description'])\n",
    "    X_train_name = vect_name_n.fit_transform(X_train['name'])\n",
    "    X_train_skills = vect_skills_n.fit_transform(X_train['key_skills'])\n",
    "    X_train_exp = scipy.sparse.csc_matrix(X_train['experience']).transpose()\n",
    "    \n",
    "    X_train_tfidf = hstack([X_train_desc, X_train_name, X_train_skills, X_train_exp])\n",
    "    \n",
    "    reg_test = SGDRegressor(max_iter=30000, learning_rate='adaptive',\n",
    "                            penalty = 'l2', random_state=rng)\n",
    "    reg_test.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    X_test_desc = vect_desc_n.transform(X_test['description'])\n",
    "    X_test_name = vect_name_n.transform(X_test['name'])\n",
    "    X_test_skills = vect_skills_n.transform(X_test['key_skills'])\n",
    "    X_test_exp = scipy.sparse.csc_matrix(X_test['experience']).transpose()\n",
    "    X_test_tfidf = hstack([X_test_desc, X_test_name, X_test_skills, X_test_exp])\n",
    "    \n",
    "    pred = reg_test.predict(X_test_tfidf)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    print(f'N_gramm: {n}, R2: {r2:.4f}')\n",
    "    \n",
    "    if r2 > best_r2: best_r2, best_ngram = r2, n \n",
    "    \n",
    "    scores = cross_val_score(reg_test, X_train_tfidf, y_train,\n",
    "                         scoring='r2', cv=5, n_jobs=-1)\n",
    "\n",
    "    print(f'Cross-val scores: {scores}')\n",
    "    print(f'Average cval score: {np.mean(scores):.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60f613",
   "metadata": {},
   "source": [
    "Использование юниграм+биграм дает прирост, попробуем оставить кодировку по словам с юни+биграммами для описаний, и использовать кодировку по буквенным сочетаниям для названия и кючевых навыков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c937624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для прогона разных вариаций векторизации:\n",
    "# Добавение колонки с опытом дает прирост (тестил отдельно, убрал чтобы\n",
    "# не раздвать тетрадку), но прилично увеличивает время на расчеты, пока мы\n",
    "# перебираем варианты н-грамм и анализаторов, я уберу ее из расчетов\n",
    "\n",
    "def vect_test(X_train, X_test, y_train, y_test, analyzer, n_words=None, n_chars=None):\n",
    "    \n",
    "    \"\"\"Изменяем параметры анализа: перебираем анализатор\n",
    "    (по слову, по буквам, по буквам с ограничением (токены только внутри слов),\n",
    "    и вариации н-грамм для названия и ключевых навыков.\"\"\"\n",
    "    \n",
    "    vect_desc_n = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=2)\n",
    "    if analyzer == 'word':\n",
    "        vect_name_n = TfidfVectorizer(analyzer=analyzer, ngram_range=n_words, min_df=2)\n",
    "        vect_skills_n = TfidfVectorizer(analyzer=analyzer, ngram_range=n_words, min_df=2)\n",
    "    else:\n",
    "        vect_name_n = TfidfVectorizer(analyzer=analyzer, ngram_range=n_chars, min_df=2)\n",
    "        vect_skills_n = TfidfVectorizer(analyzer=analyzer, ngram_range=n_chars, min_df=2)\n",
    "        \n",
    "    X_train_desc = vect_desc_n.fit_transform(X_train['description'])\n",
    "    X_train_name = vect_name_n.fit_transform(X_train['name'])\n",
    "    X_train_skills = vect_skills_n.fit_transform(X_train['key_skills'])\n",
    "    X_train_tfidf = hstack([X_train_desc, X_train_name, X_train_skills])\n",
    "    \n",
    "    reg_test = SGDRegressor(max_iter=30000, learning_rate='adaptive',\n",
    "                            penalty = 'l2', random_state=rng)\n",
    "    reg_test.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    X_test_desc = vect_desc_n.transform(X_test['description'])\n",
    "    X_test_name = vect_name_n.transform(X_test['name'])\n",
    "    X_test_skills = vect_skills_n.transform(X_test['key_skills'])\n",
    "    X_test_tfidf = hstack([X_test_desc, X_test_name, X_test_skills])\n",
    "\n",
    "    pred = reg_test.predict(X_test_tfidf)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    \n",
    "    scores = cross_val_score(reg_test, X_train_tfidf, y_train,\n",
    "                         scoring='r2', cv=5, verbose=0, n_jobs=-1)\n",
    "    \n",
    "\n",
    "    return r2, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_words = [(1,1), (1,2), (1,3)]\n",
    "n_chars = [(1,3), (1,5), (1,7), (1,9), (2,5), (2,9)]\n",
    "analyzers = ['word', 'char', 'char_wb']\n",
    "r2s = []\n",
    "scoress = []\n",
    "best_cv = 0\n",
    "for analyzer in analyzers:\n",
    "    if analyzer == 'word':\n",
    "        for words in n_words:\n",
    "            r2, scores = vect_test(X_train, X_test, y_train, y_test,\n",
    "                             analyzer=analyzer, n_words=words, n_chars=None)\n",
    "            print(analyzer, words, r2, scores, np.mean(scores))\n",
    "            if np.mean(scores) > best_cv:\n",
    "                best_cv, params = np.mean(scores), [analyzer, words]\n",
    "                \n",
    "    else: \n",
    "        for chars in n_chars:\n",
    "            r2, scores = vect_test(X_train, X_test, y_train, y_test,\n",
    "                             analyzer=analyzer, n_words=None, n_chars=chars)\n",
    "            print(analyzer, chars, r2, scores, np.mean(scores))\n",
    "            if np.mean(scores) > best_cv:\n",
    "                best_cv, params = np.mean(scores), [analyzer, chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv, params, baseline_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02069803",
   "metadata": {},
   "source": [
    "Теперь соберем вместе лучшие параметры векторизации + данные об опыте (уменьшу количество фолдов на кросс валидации, иначе при 5, очень долго ждать схождения, но при 5 он показывал средний скор близкий к 49):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_desc_n = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=2)\n",
    "vect_name_n = TfidfVectorizer(analyzer='char_wb', ngram_range=(1,3), min_df=2)\n",
    "vect_skills_n = TfidfVectorizer(analyzer='char_wb', ngram_range=(1,3), min_df=2)\n",
    "\n",
    "X_train_desc = vect_desc_n.fit_transform(X_train['description'])\n",
    "X_train_name = vect_name_n.fit_transform(X_train['name'])\n",
    "X_train_skills = vect_skills_n.fit_transform(X_train['key_skills'])\n",
    "X_train_exp = scipy.sparse.csc_matrix(X_train['experience']).transpose()\n",
    "\n",
    "X_train_tfidf = hstack([X_train_desc, X_train_name, X_train_skills, X_train_exp])\n",
    "\n",
    "reg_test = SGDRegressor(max_iter=40000, learning_rate='adaptive',\n",
    "                        penalty = 'l2', random_state=rng)\n",
    "reg_test.fit(X_train_tfidf, y_train)\n",
    "\n",
    "X_test_desc = vect_desc_n.transform(X_test['description'])\n",
    "X_test_name = vect_name_n.transform(X_test['name'])\n",
    "X_test_skills = vect_skills_n.transform(X_test['key_skills'])\n",
    "X_test_exp = scipy.sparse.csc_matrix(X_test['experience']).transpose()\n",
    "X_test_tfidf = hstack([X_test_desc, X_test_name, X_test_skills, X_test_exp])\n",
    "\n",
    "pred = reg_test.predict(X_test_tfidf)\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'R2: {r2:.4f}')\n",
    "\n",
    "scores = cross_val_score(reg_test, X_train_tfidf, y_train,\n",
    "                     scoring='r2', cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "print(f'Cross-val scores: {scores}')\n",
    "print(f'Average cval score: {np.mean(scores):.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee91c18",
   "metadata": {},
   "source": [
    "Лучшим вариантом TF-IDF векторизации оказалось использование векторазации колонки описания по словам с юни и би-граммами, и колонок названий и ключевых навыков по буквенным сочетаниям (в пределах слова) с юни, би и три-граммами. Этот результат всего лишь на 3 (примерно) пункта R2 отстает от первого места с соревнования, где использовали BERT для получения эмбедингов, и хоть и простенькую, но все же нейронку для предсказания. Очень неплохо для простой по сути модели векторизации и довольно привычной регрессии.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e738c8",
   "metadata": {},
   "source": [
    "#### Градиентный бустинг:\n",
    "Прежде чем переходить к Word2Vec, попробуем скормить лучший вариант TF-IDF векторизации катбусту:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74371338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Вначале базовый вариант:\n",
    "base_cat = CatBoostRegressor(eval_metric='R2')\n",
    "base_cat.fit(X_train_tfidf, y_train, verbose=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = base_cat.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = r2_score(y_test, pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12941e6a",
   "metadata": {},
   "source": [
    "Хороший результат, но какое же долгое даже базовое обучение, подбор гиперпараметров может занять крайне много времени, попробуем снизить размерность данных, и скормить их библиотеке Optuna для оптимизации гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19c9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем уменьшить размерность, чтобы ускорить обучение:\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11aa70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перезададим базовые вектора (без колонки опыта)\n",
    "vect_desc_n = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=2)\n",
    "vect_name_n = TfidfVectorizer(analyzer='char_wb', ngram_range=(1,3), min_df=2)\n",
    "vect_skills_n = TfidfVectorizer(analyzer='char_wb', ngram_range=(1,3), min_df=2)\n",
    "X_train_desc = vect_desc_n.fit_transform(X_train['description'])\n",
    "X_train_name = vect_name_n.fit_transform(X_train['name'])\n",
    "X_train_skills = vect_skills_n.fit_transform(X_train['key_skills'])\n",
    "X_train_tfidf = hstack([X_train_desc, X_train_name, X_train_skills])\n",
    "\n",
    "X_test_desc = vect_desc_n.transform(X_test['description'])\n",
    "X_test_name = vect_name_n.transform(X_test['name'])\n",
    "X_test_skills = vect_skills_n.transform(X_test['key_skills'])\n",
    "X_test_tfidf = hstack([X_test_desc, X_test_name, X_test_skills])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2369b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000, random_state=rng)\n",
    "X_trans_train = svd.fit_transform(X_train_tfidf)\n",
    "X_trans_test = svd.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4ffd0",
   "metadata": {},
   "source": [
    "Попробуем использовать базовые параметры, со сниженной размерностью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_2_cat = CatBoostRegressor(eval_metric='R2')\n",
    "base_2_cat.fit(X_trans_train, y_train, verbose=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = base_2_cat.predict(X_trans_test)\n",
    "score = r2_score(y_test, pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d635d",
   "metadata": {},
   "source": [
    "Мда, теряется действительно много, теперь попробуем подобрать параметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cb9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовая функция для подбора параметров, пробовл перебирать очень много чего:\n",
    "def objective(trial):\n",
    "    \n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1.0),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 5000)\n",
    "#         'iterations':trial.suggest_int(\"iterations\", 4000, 25000),\n",
    "#         'od_wait':trial.suggest_int('od_wait', 500, 2300),\n",
    "#         'learning_rate' : trial.suggest_uniform('learning_rate',0.001, 1),\n",
    "#         'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n",
    "#         'subsample': trial.suggest_uniform('subsample',0,1),\n",
    "#         'random_strength': trial.suggest_uniform('random_strength',10,50),\n",
    "#         'depth': trial.suggest_int('depth',1, 15),\n",
    "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,50),\n",
    "#         'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n",
    "    }\n",
    "\n",
    "    model = catboost.CatBoostRegressor(\n",
    "        eval_metric='R2',\n",
    "        random_state=rng,\n",
    "        **params,\n",
    "    )\n",
    "    model.fit(X_trans_train, y_train)\n",
    "    score = model.score(X_trans_test, y_test)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаем подбор параметров, вообще хочется поставить количиство попыток\n",
    "# хотя бы 100, но даже 20 занимают долгое время, а катбуст не поддерживает расчеты\n",
    "# на AMD GPU (в отличие например от торча).\n",
    "# Этот подбор занял около 2 часов.\n",
    "\n",
    "study_cat = optuna.create_study(direction=\"maximize\")\n",
    "study_cat.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "study_cat.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = CatBoostRegressor(**study_cat.best_params)\n",
    "best_model.fit(trans, y_train, verbose=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_model.predict(trans_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = r2_score(y_test, pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_model('best_boost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d30358",
   "metadata": {},
   "source": [
    "Мда, 2 часа обучения в пустую. Подбор параметров улучшил базовый результат (до 0.38). Снижение размерности сильно портит результат, а без него подбор парметров растягвается очень на долго. Ладно, как по мне, в данной задачи с такими векторами, стохастическая регрессия себя прекрасно показала, лучше СНАЧАЛА потратить время на дополниельные модификации признаков (параметры векторизации, и комбинации), чем тратить ночи (оптимизация реально могла идти всю ночь если оставить больше параметров) на катбуст, получая в итоге чуть худший результат.\n",
    "Попробуем более продвинутый способ векторизации:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530e5be",
   "metadata": {},
   "source": [
    "### Word2Vec \n",
    "В отличие от TF-IDF, Word2Vec выдает векторы одинаковой длинны для каждого слова, чаще всего используюся векторы длинны 300. Эти вектора обозначают некое значение каждого слова по 300 разным параметрам, эта можель уже улавливает семантику. Под капотом там простая 1 слойная нейронка, которая на вход получает двигающееся окно из нескольких слов. Обучается может 2 путями - Continious Bag of Words (CBOW), и Skip-Gram. В первом случае она учится предсказывать центральное слово в движущемся окне, на основе окружающих его слов. Во втором, учится предсказывать окружающие на основе данного центрального. Gensim построен на скип-граме (которая в целом используется чаще чем cbow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для обучения совместим все:\n",
    "X_train_w2v = X_train['name'] + ' ' + X_train['description'] + ' ' + X_train['key_skills']\n",
    "\n",
    "# список токенов для обучения\n",
    "w2vec_tokens = [nltk.word_tokenize(element) for element in X_train_w2v]\n",
    "\n",
    "# обучаем word2vec, гиперпараметры подбирал раньше, это одни из оптимальных для этой задачи\n",
    "\n",
    "w2v = Word2Vec(w2vec_tokens, sg=1, hs=1,\n",
    "                     vector_size=300, window=9, min_count=1, workers=16, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['грузчик'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e3a2a",
   "metadata": {},
   "source": [
    "Мы получили эмбеддинги для всех слов, но нам нужны эмбеддинги всего описания, попробуем взять взвешенный средний эмбеддинг по всем словам к каждом описании, где весом слова будет его TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb66a5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Берем веса из базового tfidf высчитанного ранее, поскольку word2vec считает только\n",
    "# слова, всякие биграммы и токенизация по буквенным сочетаниям нам тут не нужна,\n",
    "# поэтоум берем базовый:\n",
    "weights = dict(zip(baseline_tfidf.get_feature_names_out(), baseline_tfidf.idf_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция расчитывающая эмбеддинг для всего описания + название + навыки\n",
    "\n",
    "def w2v_vects(texts):\n",
    "    w2v_features = []\n",
    "    words_count = 0\n",
    "    for element in texts:\n",
    "        vect = np.zeros(300)\n",
    "        tokens = nltk.word_tokenize(element)\n",
    "        for token in tokens:\n",
    "            words_count += 1\n",
    "            if (token in w2v.wv.key_to_index.keys() and (token in weights)):\n",
    "                word_vector = w2v.wv[token]\n",
    "                vect = vect + word_vector * weights[token]\n",
    "        \n",
    "        if words_count == 0:\n",
    "            words_count = 1\n",
    "        w2v_features.append(vect/words_count)\n",
    "        words_count = 0\n",
    "                \n",
    "    return w2v_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9150671",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_w2v_train = w2v_vects(X_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_w2v = X_test['name'] + ' ' + X_test['description'] + ' ' + X_test['key_skills']\n",
    "features_w2v_test = w2v_vects(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_w2v_train = pd.DataFrame(features_w2v_train)\n",
    "features_w2v_test = pd.DataFrame(features_w2v_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e097d",
   "metadata": {},
   "source": [
    "Попробуем использовать базовую модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c455ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_test = SGDRegressor(max_iter=30000, learning_rate='adaptive',\n",
    "                            penalty = 'l2', random_state=rng)\n",
    "    \n",
    "reg_test.fit(features_w2v_train, y_train)\n",
    "\n",
    "\n",
    "scores = cross_val_score(reg_test, features_w2v_train, y_train,\n",
    "                     scoring='r2', cv=5, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82af545c",
   "metadata": {},
   "source": [
    "Здесб уже регрессия начинает выдавать дичь, вероятно можно поиграться с нормированием признаков, и зафайнтюнить регрессию так чтобы он дала нормальный результат, НО при этом катбуст спрвляется сразу, и учится во много раз быстрее чем с TF-IDF эмбеддингами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cat = CatBoostRegressor(eval_metric='R2', learning_rate=0.06, n_estimators=3000)\n",
    "base_cat.fit(features_w2v_train, y_train, silent=True, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = base_cat.predict(features_w2v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5822fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = r2_score(y_test, pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb46599",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result =  cross_val_score(base_cat, features_w2v_train, y_train, cv=5)\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовая функция для подбора параметров, пробовл перебирать очень много чего:\n",
    "def objective(trial):\n",
    "    \n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.03, 1.0),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 5000)\n",
    "\n",
    "    }\n",
    "\n",
    "    model = catboost.CatBoostRegressor(\n",
    "        eval_metric='R2',\n",
    "        random_state=rng,\n",
    "        **params,\n",
    "    )\n",
    "    model.fit(features_w2v_train, y_train)\n",
    "    score = model.score(features_w2v_test, y_test)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae91354",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_cat = optuna.create_study(direction=\"maximize\")\n",
    "study_cat.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "study_cat.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a538dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = CatBoostRegressor(**study_cat.best_params)\n",
    "best_model.fit(features_w2v_train, y_train, verbose=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec03440",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_model.predict(features_w2v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = r2_score(y_test, pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694da41",
   "metadata": {},
   "source": [
    "#### Заключение:\n",
    "Лучший результат в 0.48, на кросс валидации и 0.49 на тесте от TF-IDF склеенного с опытом работы, и скормленного стохастической линейной регрессии с l-2 регуляризатором. Бейзлайн давал примерно 0.445 пункта везде. Подозреваю что можно подкрутить Word2Vec, поскольку основная егго проблема тут - выборка для обучения маловата. Для хорошего word2vec нужен огромный корпус. Но все равно, в этой задаче TF-IDF + SGDRegressor показывают себя очень хорошо, близко к существенно более тяжеловесным моделям. Подозреваю что это из-за специфики задачи, тут не важен тон/настроение, еще какие либо характеристки текста, которые могут уловить трансформеры, да и прогноз 1й цифры очень хорошо выполняется регрессией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e202510e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
